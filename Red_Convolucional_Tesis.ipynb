{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf # se importa tensor flow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# funcion  para importar la data de tensorflow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#inicio session interactiva\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construccion del modelo softmax  con una sola capa \n",
    "#************se construyen los nodos de la imagen de entrada*********************\n",
    "# x imagenes de entrada y y imagenes de salida \n",
    "#\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784]) # variables simbolicas para manipulacion \n",
    "# x es un tensor  2d  punto flotante , 784 dimensiones de una unica imagen \n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "# y tensor 2d punto flotante , clases de salida 10 dimensiones indica la clase del digito a la cual la imagen pertenece\n",
    "# el shaspe Inserta un marcador de posición para un tensor que será siempre alimentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#definciion de pesos w y un sesgo b tensorflow tiene la propieda de agragarlos a una variable que vive en todo el grafico con \n",
    "# tf.variable  donde se inicilizan como tensores llenos de ceros (W y B) \n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "# b es un vector de 10 dimensiones por que se tienen 10 clases\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "# w es una matriz de 784x10 (784 entidades y 10 de salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables()) # inicializacion de variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# funcion de coste (hasta ahora solo es un modelo de regresion)\n",
    "#**************importante************\n",
    "# se hace uso de una funcion lienal \n",
    "# 1) se multiplican las imagenes puestas anteriormente en un vector (kernel 3) por la matriz de pesos w mas el sesgo b (bias)**\n",
    "# y se calcula la probabilidad softmax que asigna la probalidad de pertenencia a cada clase\n",
    "# normalliza los datos de salida para que todos me sumen 1\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross entropy reduce la funcion de coste con el entrenamiento intentar conseguir valores de los parámetros\n",
    "#W y b que minimicen el valor de la métrica que indica cuan malo es el modelo.\n",
    "# tf.reduce_sum se apliza a travez de todas las clases y tf.reduce_mean la media de todas las sumas\n",
    "# Donde y es la distribución de probabilidad precedida y la y la distribución\n",
    "#real (obtenida a partir del etiquetado de los datos de entrada) lo que busca es que ambas sean iguales \n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "# se calcula el logaritmo de cada elemento y con la función que\n",
    "#nos provee TensorFlow tf.log y después se multiplica por cada\n",
    "#elemento y_ . Finalmente, con tf.reduce_sum se suman todos los\n",
    "#elementos del tensor (más adelante veremos que las imágenes se miran\n",
    "#por paquetes, y en este caso el valor de cross-entropy corresponde a la\n",
    "#suma de las cross-entropy de las imágenes de un paquete y no a la de\n",
    "#una sola)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uso del gradiente descediente \n",
    "train_step = tf.train.GradientDescentOptimizer(0.10).minimize(cross_entropy)\n",
    "#La taza de aprendizaje es una constante que indica qué tan grande haremos un paso en la dirección opuesta al gradiente\n",
    "#GradientDescentOptimizer(learning rate) \n",
    "# TensorFlow tiene una variedad de algoritmos de optimización orden interna\n",
    "# con una longitud de paso de 0,5, para minimiazar el cross entropy.\n",
    "# elñ gradiente calcular los pasos de actualización de parámetros, y aplicar medidas de actualización de los parámetros.\n",
    "# se aproxima de forma negativa al gradiente busca el minimo global \n",
    "#en que punto empieza mi gradiente descendiente ????? cuanto puedo ir avanzadano "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rabdi de cada 1000 iteraciones entrenamiento se genera  por la ejecucion repetida de cada paso\n",
    "for i in range(1000):\n",
    "  batch = mnist.train.next_batch(50) # 50 ejemplos \n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})# feed_dict reemplaza los tensores de marcador de posición\n",
    "     #x e Y con los ejemplos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf.argmax es una función extremadamente útil que le da el índice de la entrada más alta de un tensor a lo largo de un eje\n",
    "accert_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "# etiqueta que retona el sistema tf.argmax(y,1), etiqueta verdadera tf.argmax(y_,1)\n",
    "# retorna bool\n",
    "#verifica\n",
    "# 2 funciones de perdida o cosot guia para incognitas desempeñ mide solo que tan bueno fue el soluciones  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9081\n"
     ]
    }
   ],
   "source": [
    "# se determina la media de los puntos flotantes y luego se toma la media \n",
    "accuracy = tf.reduce_mean(tf.cast(accert_prediction, tf.float32)) \n",
    "print (accuracy.eval(feed_dict ={x : mnist.test.images,y_ : mnist.test.labels}))\n",
    "# cuantas veces fue acertada\n",
    "# como arreglar el modelo ? red convolucional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construccion de red convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no comprendo muy bien \n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "# no ceros valores aleatorio a una matriz truncate normal deviacion shape\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "# todos los valores en 0.1\n",
    "\n",
    "# solo los valores en cero empezar en cero lejano \n",
    "# incia busqu de parametros en punto aleatorio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maxpooling de 2x2.\n",
    "#funciones de tensor\n",
    "#filtros W\n",
    "#entradas \n",
    "#tamño de paso strides  que tanto me muevo en las posiciones del vecrotr que entra \n",
    "#doc con2d\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# procesamiwento de todos los canales en cada uno hace transformacion convolucionales tien un w y un b\n",
    "\n",
    "# despuesd etener reformada la imagen \n",
    "# se activa pagtron el filtro y el pixel filtro pesos \n",
    "# remplaza por\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME')# COMO DEFINIO EL MAX POOLING\n",
    "#solo declaraciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# particiones de la imagen ------------------- ????????????????????????????\n",
    "# La convolución computará 32 característica para cada sector [5, 5] \n",
    "# 1 canales de entrada \n",
    "# numero de canales de salida\n",
    "# tensor de peso [5, 5, 1, 32]\n",
    "W_conv1 = weight_variable([5, 5, 1, 32]) # PRIMERA CONVOLUCION \n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#para aplicar hay que modelar x a un tensor de 4d 2,3 alto y ancho y ultimo canal de color \n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "# no vector se coamonadan pixeles en 2d\n",
    "#- muchas imagen pixeles, 1 canal de color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#se aplica convoluvion a x_image, se añadeel sesgo  y se aplica relu y a esta misma el maxpooling\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # derivada\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# segunda red convolucional \n",
    "#64 características para cada parche 5x5.\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "# canales empiezan a tener patrones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# POR QUE SE REDUCE A 7X7 LA IMAGEN???\n",
    "#añadimos una capa plenamente conectado con 1024 neuronas para permitir el procesamiento en la imagen entera\n",
    "#Nos remodelar el tensor de la capa de la agrupación en un lote de vectores\n",
    "#se multiplica por una matriz de pesos, añadir un sesgo, y aplicar una regla.\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "#capa lineal \n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "# toma todos loa  valres de la conv linelamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NO COMPRENDO MUY BIEN ?????????????\n",
    "#Para minimizar el overfitting, se aplica dropout depues de la capa de lectura\n",
    "# se crea un marcador de posicion para la probabilidad de salida para que mantenga el porcentaje de entrenamiento\n",
    "#maneja el escalado de las salidas\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "#dropout evita malas solucions , durante entrenamiento , ignora algunos full connected, conservo % que indque la proalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sea aplica softmax a la capa de regresion \n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "# 2 capa fullconnec que me lo envia en alguna de las clases\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08\n",
      "step 300, training accuracy 0.94\n",
      "step 600, training accuracy 0.94\n",
      "step 900, training accuracy 0.98\n",
      "step 1200, training accuracy 0.98\n",
      "step 1500, training accuracy 0.96\n",
      "step 1800, training accuracy 0.98\n",
      "step 2100, training accuracy 0.96\n",
      "step 2400, training accuracy 1\n",
      "step 2700, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3300, training accuracy 0.98\n",
      "step 3600, training accuracy 0.98\n",
      "step 3900, training accuracy 0.98\n",
      "step 4200, training accuracy 0.98\n",
      "step 4500, training accuracy 1\n",
      "step 4800, training accuracy 0.98\n",
      "step 5100, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5700, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6300, training accuracy 0.98\n",
      "step 6600, training accuracy 1\n",
      "step 6900, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7500, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8100, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8700, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9300, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9900, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10500, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11100, training accuracy 0.98\n",
      "step 11400, training accuracy 0.98\n",
      "step 11700, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12300, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12900, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13500, training accuracy 0.98\n",
      "step 13800, training accuracy 1\n",
      "step 14100, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14700, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15300, training accuracy 0.98\n",
      "step 15600, training accuracy 1\n",
      "step 15900, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16500, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17100, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17700, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18300, training accuracy 1\n",
      "step 18600, training accuracy 0.98\n",
      "step 18900, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19500, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.9927\n"
     ]
    }
   ],
   "source": [
    "# se usa ADAM  en vez de gradiente descenienmte y se almacenan los registros en cada iteración número100.\n",
    "#El valor keep_prob se utiliza para controlar la tasa de\n",
    "#perdida utilizado cuando se entrena la red neuronal. lo cual lleva que cada conexión\n",
    "#entre las capas (en este caso entre la última capa densamente conectado y la capa de lectura) sólo será\n",
    "#utilizada con una probabilidad de 0,5 cuando el entrenamiento. Esto reduce sobreajuste.\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))#funcion de costo \n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) \n",
    "accert_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))# funcion de desempeño\n",
    "accuracy = tf.reduce_mean(tf.cast(accert_prediction, tf.float32))# funcion de desempeño \n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(20000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%300 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t =batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d=np.reshape(t[:],[28,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
